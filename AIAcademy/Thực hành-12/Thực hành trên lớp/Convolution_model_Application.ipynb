{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: Application\n",
    "\n",
    "Trong notebook này ta sẽ:\n",
    "\n",
    "- Lập trình các hàm hỗ trợ khi xây dựng model bằng Tensorflow\n",
    "- Thực hiện training bằng CNN với TensorFlow \n",
    "\n",
    "**Sau khi hoàn thành bài tập này, bạn sẽ có thể:**\n",
    "\n",
    "- Xây dựng và huấn luyện mạng CNN cho các bài toán phân loại"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 - TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from cnn_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chạy Cell dưới đây để load dataset \"SIGNS\" dùng cho bài toán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (signs)\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset SIGNS chứa 6 kí hiệu tay đại diện cho 6 con số từ 0 to 5. \n",
    "<img src=\"images/SIGNS.png\" style=\"width:800px;height:300px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a picture\n",
    "index = 6\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên, ta sẽ xem qua kích thước của dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n",
    "conv_layers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1 - Create placeholders\n",
    "\n",
    "TensorFlow yêu cầu chúng ta tạo ra các placeholder để nạp dữ liệu vào khi chạy session.\n",
    "\n",
    "**Exercise**: Lập trình hàm để tao placeholder cho ảnh đầu vào X và nhãn Y. Bạn chưa nên fix cứng số bản ghi tại thời điểm này. Dùng \"None\" để đại diện cho kích thước của batch_size sẽ giúp ta thay đổi kích thước batch huấn luyện một cách dễ dàng. Vì thế X sẽ có kích thước là **[None, n_H0, n_W0, n_C0]** và Y sẽ có kích thước là **[None, n_y]**.  [Hint](https://www.tensorflow.org/api_docs/python/tf/placeholder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_placeholders\n",
    "\n",
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "\n",
    "    Arguments:\n",
    "    n_H0 -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "\n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈2 lines)\n",
    "    X = tf.placeholder(name=\"X\",dtype=tf.float32,shape=[None,n_H0,n_W0,n_C0])\n",
    "    Y = tf.placeholder(name= \"Y\",dtype=tf.float32,shape=[None,n_y])\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = create_placeholders(64, 64, 3, 6)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "<td>\n",
    "    X = Tensor(\"Placeholder:0\", shape=(?, 64, 64, 3), dtype=float32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "    Y = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=float32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Khởi tạo tham số\n",
    "\n",
    "Ta sẽ khởi tạo tham số/bộ lọc $W1$ and $W2$ với `tf.contrib.layers.xavier_initializer(seed = 0)`. Chúng ta không cần quan tâm đến bias vì nó sẽ được khởi tạo tự động bởi Tensorflow. Đồng thời, ta sẽ chỉ cần khởi tạo tham số cho những hàm conv2d. Những tầng fully connected được khởi tạo tham số một cách tự động.\n",
    "\n",
    "**Bài tập:** Lập trình hàm initialize_parameters(). Kích thước của mỗi nhóm filters được cho dưới đây. Để khởi tạo một tham số  $W$ có kích thước [1,2,3,4] trong Tensorflow, ta dùng:\n",
    "```python\n",
    "W = tf.get_variable(\"W\", [1,2,3,4], initializer = ...)\n",
    "```\n",
    "[More Info](https://www.tensorflow.org/api_docs/python/tf/get_variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [4, 4, 3, 8]\n",
    "                        W2 : [2, 2, 8, 16]\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "\n",
    "    tf.set_random_seed(1)  # so that your \"random\" numbers match ours\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 lines of code)\n",
    "    W1 = None\n",
    "    W2 = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    parameters = initialize_parameters()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"].eval()[1,1,1]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"].eval()[1,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "W1[1,1,1] = \n",
    "[ 0.00131723  0.14176141 -0.04434952  0.09197326  0.14984085 -0.03514394\n",
    " -0.06847463  0.05245192]\n",
    "W1.shape: (4, 4, 3, 8)\n",
    "\n",
    "\n",
    "W2[1,1,1] = \n",
    "[-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058\n",
    " -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228\n",
    " -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n",
    "W2.shape: (2, 2, 8, 16)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Forward propagation\n",
    "\n",
    "Trong TensorFlow, ta có những hàm build sẵn giúp ta thực hiện bước convolution.\n",
    "\n",
    "- **tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = 'SAME'):** với một input $X$ và một bộ lọc $W1$, hàm này thực hiện phép nhân tích chập giữa $W1$ và X. Đầu vào thứ ba, ([1,f,f,1]) đại diện cho số bước nhảy với mỗi kích thước đầu vào (m, n_H_prev, n_W_prev, n_C_prev). Đọc thêm [tại đây](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)\n",
    "\n",
    "- **tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = 'SAME'):** cho đầu vào là A, hàm này sẽ sử dụng một cửa số có kích thước (f, f) và bước nhảy là (s, s) để thực hiện max pooling với mỗi vùng ảnh. Đọc thêm [tại đây](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool)\n",
    "\n",
    "- **tf.nn.relu(Z1):** tính toán đầu ra của hàm ReLU với ma trận đầu vào Z1 (kích thước bất kì). Đọc thêm[tại đây.](https://www.tensorflow.org/api_docs/python/tf/nn/relu)\n",
    "\n",
    "- **tf.contrib.layers.flatten(P)**: Với dữ liệu đầu vào P, hàm này sẽ duỗi mỗi bản ghi thành một vector một chiều, và giữ nguyên batch size, đầu ra là một tensor với kích thước [batch_size, k]. Đọc thêm [tại đây.](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten)\n",
    "\n",
    "- **tf.contrib.layers.fully_connected(F, num_outputs):** Cho một đầu và đã được flatten F, hàm sẽ tạo ra một layer fully connected với số neuron = num_outputs. Đọc thêm [tại đây.](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected)\n",
    "\n",
    "Trong hàm cuối cùng phía trên (`tf.contrib.layers.fully_connected`), tầng fully connected tự động khởi tạo tham số trong đồ thị và liên tục cập nhật tham số trong quá trình huấn luyện. \n",
    "\n",
    "\n",
    "**Exercise**: \n",
    "\n",
    "Lập trình hàm `forward_propagation` với kiến trúc sau: `CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED`. Sử dụng các hàm được nhắc đến phía trên. \n",
    "\n",
    "Cụ thể, ta sẽ sử dụng những tham số sau cho tất cả các bước:\n",
    "     - Conv2D: stride 1, padding \"SAME\"\n",
    "     - ReLU\n",
    "     - Max pool: Bộ lọc kích thước 8x8 và bước nhảy là 8x8, padding \"SAME\"\n",
    "     - Conv2D: stride 1, padding \"SAME\"\n",
    "     - ReLU\n",
    "     - Max pool: Bộ lọc kích thước là 4 x 4 và bước nhảy là 4x4, padding \"SAME\"\n",
    "     - Flatten đầu ra của lớp max pool.\n",
    "     - FULLYCONNECTED (FC) layer: Đắp một tầng fully connected không sử dụng activation phi tuyến. Chúng ta không gọi hàm softmax ở đây. Kết quả của hàm sẽ là 6 neurons trong output layer để đưa vào softmax sau đó. Trong TensorFlow, hàm softmax và cost được tổng hợp cùng với nhau thành một hàm mà ta sẽ gọi sau. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "\n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the parameters from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # CONV2D: stride of 1, padding 'SAME'\n",
    "    Z1 = None\n",
    "    # RELU\n",
    "    A1 = None\n",
    "    # MAXPOOL: window 8x8, sride 8, padding 'SAME'\n",
    "    P1 = None\n",
    "    # CONV2D: filters W2, stride 1, padding 'SAME'\n",
    "    Z2 = None\n",
    "    # RELU\n",
    "    A2 = None\n",
    "    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    P2 = None\n",
    "    # FLATTEN\n",
    "    P2 = None\n",
    "    # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n",
    "    # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\"\n",
    "    Z3 = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    np.random.seed(1)\n",
    "    X, Y = create_placeholders(64, 64, 3, 6)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    a = sess.run(Z3, {X: np.random.randn(2,64,64,3), Y: np.random.randn(2,6)})\n",
    "    print(\"Z3 = \" + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table> \n",
    "    <td> \n",
    "    Z3 =\n",
    "    </td>\n",
    "    <td>\n",
    "    [[-0.44670227 -1.57208765 -1.53049231 -2.31013036 -1.29104376  0.46852064] <br>\n",
    " [-0.17601591 -1.57972014 -1.4737016  -2.61672091 -1.00810647  0.5747785 ]]\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Tính cost\n",
    "\n",
    "Lập trình hàm cost dưới đây. Bạn có thế sử dụng các hàm sau: \n",
    "\n",
    "- **tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y):** tính softmax entropy loss. Hàm này kết hợp cả softmax activation function với hàm loss. Đọc thêm [tại đây.](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "- **tf.reduce_mean:** tính giá trị trung bình của các phần tử theo một trục nào đấy của một tensor. Sử dụng hàm này để tính kết quả loss trung bình của tất cả các bản ghi để ra cost. Đọc thêm [tại đây.](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)\n",
    "\n",
    "**Bài tập**: Lập trình hàm cost sử dụng các hàm được gợi ý."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "\n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "\n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3,labels=Y))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    np.random.seed(1)\n",
    "    X, Y = create_placeholders(64, 64, 3, 6)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    a = sess.run(cost, {X: np.random.randn(4,64,64,3), Y: np.random.randn(4,6)})\n",
    "    print(\"cost = \" + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "```\n",
    "cost = 2.91034\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Model \n",
    "\n",
    "Cuối cùng, ta sẽ hợp nhất những hàm phía trên thành một model. Sau đó sử dụng moddel này để huấn luyện với dataset SIGNS. \n",
    "\n",
    "\n",
    "**Bài tập**: Hoàn thiện hàm dưới đây. \n",
    "\n",
    "Model hoàn chỉnh sẽ có những phần sau:\n",
    "\n",
    "- create placeholders\n",
    "- initialize parameters\n",
    "- forward propagate\n",
    "- compute the cost\n",
    "- create an optimizer\n",
    "\n",
    "Với mỗi epoch trong quá trình huấn luyện, ta sẽ lấy ra những mini-batch để huấn luyện và cập nhật trọng số theo từng batch. [Hint for initializing the variables](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate=0.009,\n",
    "          num_epochs=100, minibatch_size=64, print_cost=True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer ConvNet in Tensorflow:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "\n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_train -- test set, of shape (None, n_y = 6)\n",
    "    X_test -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_test -- test set, of shape (None, n_y = 6)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "\n",
    "    Returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    ops.reset_default_graph()  # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)  # to keep results consistent (tensorflow seed)\n",
    "    seed = 3  # to keep results consistent (numpy seed)\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape\n",
    "    n_y = Y_train.shape[1]\n",
    "    costs = []  # To keep track of the cost\n",
    "\n",
    "    # Create Placeholders of the correct shape\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    parameters = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z3 = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "\n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size)  # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                \n",
    "\n",
    "                ### END CODE HERE ###\n",
    "\n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print(\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(Z3, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chạy đoạn code dưới với 100 epochs. Kiểm tra giá trị cost sau epoch 0 và 5 có đúng với expected output không. Nếu không, bạn nên ngừng chạy và quay lại kiểm tra code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**: kết quả đầu ra nên khá gần tương tự với những giá trị sau.\n",
    "\n",
    "- **Cost after epoch 0 =** 1.917929\n",
    "- **Cost after epoch 5 =** 1.506757\n",
    "- **Train Accuracy   =** 0.940741\n",
    "- **Test Accuracy   =**  0.783333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúc mừng! Bây giờ bạn đã biết xây dựng một model CNN với Tensorflow. Nếu muốn, bạn có thể hiệu chỉnh tham số để cải độ chính xác.\n",
    "\n",
    "Thumb up to your work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "fname = \"images/thumbs_up.jpg\"\n",
    "image = cv2.imread(fname)\n",
    "my_image = cv2.resize(image, dsize=(64,64))\n",
    "plt.imshow(cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB))"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "bwbJV",
   "launcher_item_id": "0TkXB"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
